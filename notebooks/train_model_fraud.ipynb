{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import datarobot as dr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# The notebook should be executed from the project root directory\n",
    "if \"_correct_path\" not in locals():\n",
    "    os.chdir(\"..\")\n",
    "    sys.path.append(\".\")\n",
    "    print(f\"changed dir to {Path('.').resolve()})\")\n",
    "    _correct_path = True\n",
    "load_dotenv()\n",
    "client = dr.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from infra.settings_main import project_name\n",
    "from nbo.custom_metrics import metrics_manager\n",
    "from nbo.schema import (\n",
    "    AppDataScienceSettings,\n",
    "    AppInfraSettings,\n",
    "    LLMModelSpec,\n",
    "    OutcomeDetail,\n",
    "    association_id,\n",
    ")\n",
    "\n",
    "custom_metric_baselines = metrics_manager.get_baseline_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_ds_settings = AppDataScienceSettings(\n",
    "    page_title=\"Fraud Monitor Officer\",\n",
    "    page_subtitle=textwrap.dedent(\"\"\"\\\n",
    "        Combine predictive and generative AI to help review customer transaction alerts.\n",
    "        All you need to do is choose a transaction and hit submit!\"\"\"),\n",
    "    record_identifier={\n",
    "        \"column_name\": \"record_id\",\n",
    "        \"display_name\": \"Transaction ID\",\n",
    "    },\n",
    "    text_explanation_feature=\"csrNotes\",  # Optional, include when a text variable is present and ngrams feature explanations are desired\n",
    "    no_text_gen_label=None,  # Optional, include in predictions where an email is not desired\n",
    "    default_number_of_explanations=3,\n",
    "    target_probability_description=\"the likelihood of fraudulent activity\",  # Preceeded in prompt by \"Feature is increasing/decreasing\"\n",
    "    email_prompt=textwrap.dedent(\"\"\"\\\n",
    "        Draft an email to a fraud analyst that the transaction was {prediction_label}.\n",
    "        The email should refer to the transaction id {selected_record} and contain a subject line and body.\n",
    "        Keep the email tone {tone}. Keep the length {verbosity}. Try not to use many emojis.\n",
    "\n",
    "        Incorporate the following list of factors that influenced the determination:\n",
    "\n",
    "        {rsp}\"\"\"),\n",
    "    outcome_details=[  # Each item should be a target value with it's corresponding label and optional description\n",
    "        OutcomeDetail(\n",
    "            prediction=0,\n",
    "            label=\"Non-fraudulent\",\n",
    "            description=\"The transaction is not suspicious and does not require further investigation.\",\n",
    "        ),\n",
    "        OutcomeDetail(\n",
    "            prediction=1,\n",
    "            label=\"Suspicious activity\",\n",
    "            description=\"The transaction requires a detailed review by the fraud investigative analyst.\",\n",
    "        ),\n",
    "    ],\n",
    "    custom_metric_baselines=custom_metric_baselines,\n",
    "    association_id_column_name=association_id,\n",
    "    tones=[\n",
    "        \"authoritative and expert\",\n",
    "        \"educational and informative\",\n",
    "        \"formal and elevated\",\n",
    "        \"friendly and casual\",\n",
    "        \"lighthearted and funny\",\n",
    "        \"witty and playful\",\n",
    "    ],\n",
    "    verbosity=[\"short and sweet\", \"normal\", \"long and detailed\"],\n",
    "    system_prompt=textwrap.dedent(\"\"\"\\\n",
    "        You are a fraud monitoring specialist named Bob and you work for BankCorp., a regional bank.\n",
    "        Your job is to review customer transactions alerts and determine if they are suspicious or fraudulent.\n",
    "        You have detailed information about each of your customers and their transaction behavior.\n",
    "        You also have a list of important factors that explain why a particular transaction may be suspicious.\n",
    "        The goal is to incorporate these factors into a report addressed to a fraud investigative analyst\n",
    "        in order to to decide if a Suspicious Acitivty Report (SAR) should be filed.\"\"\"),\n",
    "    model_spec=LLMModelSpec(\n",
    "        input_price_per_1k_tokens=0.001,  # update these values based on the model's pricing\n",
    "        output_price_per_1k_tokens=0.002,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobot_pulumi_utils.schema.training import (\n",
    "    AdvancedOptionsArgs,\n",
    "    AnalyzeAndModelArgs,\n",
    "    AutopilotRunArgs,\n",
    ")\n",
    "\n",
    "use_case_name = f\"NBO Fraud [{project_name}]\"\n",
    "use_case_description = \"Suspicious Activity Monitoring\"\n",
    "\n",
    "dataset_name = f\"NBO Fraud Training Data [{project_name}]\"\n",
    "file_path = \"assets/fraud_monitoring_training.csv\"\n",
    "\n",
    "\n",
    "autopilotrun_args = AutopilotRunArgs(\n",
    "    name=f\"NBO Fraud AutoPilot Run [{project_name}]\",\n",
    "    analyze_and_model_config=AnalyzeAndModelArgs(\n",
    "        metric=\"LogLoss\", mode=\"quick\", target=\"SAR\", positive_class=1\n",
    "    ),\n",
    "    advanced_options_config=AdvancedOptionsArgs(seed=42),\n",
    ")\n",
    "\n",
    "registered_model_name = f\"NBO [{project_name}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobotx.idp.autopilot import get_or_create_autopilot_run\n",
    "from datarobotx.idp.datasets import get_or_create_dataset_from_file\n",
    "from datarobotx.idp.registered_model_versions import (\n",
    "    get_or_create_registered_leaderboard_model_version,\n",
    ")\n",
    "from datarobotx.idp.use_cases import get_or_create_use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating Use Case {use_case_name}\")\n",
    "\n",
    "if \"DATAROBOT_DEFAULT_USE_CASE\" in os.environ:\n",
    "    use_case_id = os.environ[\"DATAROBOT_DEFAULT_USE_CASE\"]\n",
    "else:\n",
    "    use_case_id = get_or_create_use_case(\n",
    "        endpoint=client.endpoint,\n",
    "        token=client.token,\n",
    "        name=use_case_name,\n",
    "        description=use_case_description,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating Dataset {dataset_name}\")\n",
    "dataset_id = get_or_create_dataset_from_file(\n",
    "    token=client.token,\n",
    "    endpoint=client.endpoint,\n",
    "    name=dataset_name,\n",
    "    file_path=file_path,\n",
    "    use_cases=use_case_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating Autopilot Run {autopilotrun_args.name}\")\n",
    "project_id = get_or_create_autopilot_run(\n",
    "    token=client.token,\n",
    "    endpoint=client.endpoint,\n",
    "    dataset_id=dataset_id,\n",
    "    use_case=use_case_id,\n",
    "    **autopilotrun_args.model_dump(mode=\"json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_model_id = dr.ModelRecommendation.get(project_id).model_id  # type: ignore[union-attr,attr-defined]\n",
    "\n",
    "try:\n",
    "    model = dr.Model.get(project_id, recommended_model_id)  # type: ignore[attr-defined]\n",
    "    prediction_threshold = model.get_roc_curve(\n",
    "        source=\"validation\"\n",
    "    ).get_best_f1_threshold()\n",
    "except Exception:\n",
    "    prediction_threshold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(f\"Best model: {model.model_type}\\n\\nMetrics:\")\n",
    "\n",
    "pd.DataFrame.from_records(model.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Registered Model Version...\")\n",
    "registered_model_version_id = get_or_create_registered_leaderboard_model_version(\n",
    "    token=client.token,\n",
    "    endpoint=client.endpoint,\n",
    "    model_id=recommended_model_id,\n",
    "    registered_model_name=registered_model_name,\n",
    "    prediction_threshold=prediction_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_infra_settings = AppInfraSettings(\n",
    "    registered_model_name=registered_model_name,\n",
    "    registered_model_version_id=registered_model_version_id,\n",
    "    scoring_dataset_id=dataset_id,\n",
    "    use_case_id=use_case_id,\n",
    "    project_id=project_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from infra.settings_main import (\n",
    "    model_training_output_ds_settings,\n",
    "    model_training_output_infra_settings,\n",
    ")\n",
    "\n",
    "with open(model_training_output_ds_settings, \"w\") as f:\n",
    "    yaml.safe_dump(app_ds_settings.model_dump(mode=\"json\"), f, allow_unicode=True)\n",
    "with open(model_training_output_infra_settings, \"w\") as f:\n",
    "    yaml.safe_dump(app_infra_settings.model_dump(mode=\"json\"), f, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# This is a registry of literal parameters that can be used as node inputs and/or node outputs.
#
# When defining node inputs in the pipeline, you can reference keys from this yaml to indicate
# that the pipeline should map the literal defined here to the node's input at runtime.
#
# Kedro precedent defines that the keys in this yaml can be referenced as below when defining
# node inputs:
# e.g. "params:my_key_1", "params:my_key_2.my_sub_key_2"

deploy_llm:
  dr_credential:
    name: openai_api_key # Cannot contain spaces for use in ML Ops
    credential_type: api_token
  custom_model:
    name: ${globals:project_name} llm
    target_type: TextGeneration
    prompt_feature_name: prompt
    target_feature_name: completion
    base_environment_id: 64c964448dd3f0c07f47d040 # 3.9 GenAI drop-in env
  deployment:
    name: ${globals:project_name} llm
    association_id_column_name: association_id
  registered_model_name: ${globals:project_name} llm
  custom_metrics:
    confidence:
      baseline_values:
        - value: 0.65
      description: Confidence Score
      directionality: higherIsBetter
      is_model_specific: false
      name: Confidence
      type: average
      units: Score
    llm_cost:
      baseline_values:
        - value: 0.025
      description:
        Tracks the total cost of the LLM. For this deployment, cost is reported
        directly from OpenAI.
      directionality: lowerIsBetter
      is_model_specific: false
      name: LLM Cost
      type: average
      units: Dollars
    sentiment:
      baseline_values:
        - value: 0.3
      description: Sentiment Score
      directionality: higherIsBetter
      is_model_specific: false
      name: Sentiment
      type: average
      units: Score
    reading_time:
      baseline_values:
        - value: 75
      description: Estimated Reading Time
      directionality: lowerIsBetter
      is_model_specific: false
      name: Reading Time
      type: average
      units: Seconds
    readability_level:
      baseline_values:
        - value: 50
      description: Readability Level
      directionality: higherIsBetter
      is_model_specific: false
      name: Reability
      type: average
      units: Score
    response_token_count:
      baseline_values:
        - value: 225
      description: Number of LLM app response tokens
      directionality: lowerIsBetter
      is_model_specific: false
      name: Response Token Count
      type: average
      units: Count
    prompt_token_count:
      baseline_values:
        - value: 225
      description: Number of LLM app prompt tokens
      directionality: lowerIsBetter
      is_model_specific: false
      name: Response Token Count
      type: average
      units: Count
    user_feedback:
      baseline_values:
        - value: 0.75
      description: User provided rating of the generated email
      directionality: higherIsBetter
      is_model_specific: false
      name: User Feedback
      type: average
      units: Upvotes
